\documentclass{amsproc}

%------------------------------
%     PREAMBLE
%------------------------------

% Packages
\usepackage{amsmath, amsthm, hyperref} 
\usepackage[usenames, dvipsnames]{xcolor}
\usepackage{lipsum}

% Hyperlink package parameters
\hypersetup{
	colorlinks=true,
	linkcolor=RoyalBlue,
	citecolor=Thistle
}

% amsthm - Declaring theorem commands
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remark}[theorem]{Remark}

% Referencing including the format name (e.g. 'Section', 'Theorem') not just the numbering
\newcommand{\fref}[2]{\hyperref[#2]{#1 \ref*{#2}}}

%------------------------------
%     DOCUMENT
%------------------------------

% Title
\title{Concentration of measure}
\author{SIAM Working Group - Spring 2019}

\begin{document}

% Title and table of contents
\maketitle
\tableofcontents

%------------------------------
%     Lecture 1
%------------------------------

\section{Introduction}
\label{sec:introduction}
	\paragraph{\textbf{Abstract}}
	In this lecture, we introduced the book's central theme, the study of random fluctuations of functions of independent random variables, along with three techniques that facilitate this study. The specific focus is on how these functions concentrate around measures of central tendency such as their mean and median. We discuss the three main methods below.

	\subsection*{Method 1: Isoperimetric Inequalities}

	Suppose $(\mathcal{X},d)$ is a metric space, $X$ is a $\mathcal{X}$-valued RV with law $P$, and $Mf(X)$ is a median of $f(X)$. Then
	\[
		P\left\{|f(X)-Mf(X)|\geq t\right\}\leq 2\alpha(t)
	\]
	where
	\[
		\alpha(t):=\sup_{A\in \mathcal{B}(\mathcal{X})\atop P(A)\geq\frac{1}{2}}P\{d(X,A)\geq t\}
	\]
	and $\mathcal{B}(\mathcal{X})$ is the Borel $\sigma$-algebra on $\mathcal{X}$. Thus, bounding $\alpha(t)$ allows one to describe how $f(X)$ concentrates around its median.

	\subsection*{Method 2: Entropy}

	Let $I\subseteq\mathbb{R}$ be an interval, $X$ a $I$-valued RV, and $\Phi:I\to\mathbb{R}$ a convex function. then the $\Phi$-entropy of $X$ is
	\[
		H_\Phi(X):=E\Phi(X)-\Phi(EX).
	\]
	If $\Phi(x)=x\log(x)$ then we write $\text{Ent}$ in place of $H_\Phi$. Bounds on this entropy can translated to bounds on the concentration of functions of random variables around their mean. For example the Gaussian logarithmic Sobolev inequality states that if $X\sim N(0,I_{n\times n})$ and $f\in C^1(\mathbb{R}^n)$ then
	\[
		\text{Ent}\left[f^2(X)\right]\leq 2E\left[\|\nabla f(X)\|^2\right].
	\]
	In turn, this implies that
	\[
		P\left\{f(X)-Ef(X)\geq t\right\}\leq e^{-\frac{t^2}{2}}
	\]
	\subsection*{Method 3: Transportation}

	One other way of deriving concentration bounds is by getting estimates of the \textit{transportation cost} between two probability measures $P$ and $Q$:
	\[
		\min_{\mathbf{P}\in\mathcal{P}(P,Q)}E_\mathbf{P}d(X,Y)
	\]
	where $d$ is some cost function, and $\mathcal{P}(P,Q)$ is the class of joint distributions of $X$ and $Y$ s.t. the marginal distribution of $X$ is $P$ and the marginal distribution of $Y$ is $Q$.

%------------------------------
%     Lecture 2
%------------------------------

\section{Basic inequalities}
\label{sec:basic_inequalites}
\paragraph{\textbf{Abstract}}
\lipsum[1]
% YOUR STUFF HERE

\begin{theorem}[Theorem name]
\label{thm:reference-name}
% YOUR STUFF HERE
\end{theorem}

\begin{remark}[Trick name]
\label{rmk:reference-name}
% YOUR STUFF HERE
\end{remark}

%------------------------------
%     APPENDIX
%------------------------------

\appendix
\newpage
\section*{Credits}
\begin{tabular}{ll}
	\fref{Lecture}{sec:introduction}
	&David Gutman
	\\
	\fref{Lecture}{sec:basic_inequalites}
	&Adrian Hagerty
\end{tabular}

\end{document}
